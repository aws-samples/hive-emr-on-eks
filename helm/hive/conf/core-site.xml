<configuration>

<property>
  <name>fs.defaultFS</name>
  <value>s3a://emr-on-eks-rss-021732063925-us-west-2/hive</value>
</property>

<property>
  <name>hadoop.security.authentication</name>
  <value>simple</value>
</property>

<property>
  <name>dfs.encryption.key.provider.uri</name>
  <value></value>
</property>

<property>
  <name>ipc.client.fallback-to-simple-auth-allowed</name>
  <value>true</value>
</property>

<!-- Cf. tez.runtime.shuffle.ssl.enable for secure shuffle in tez-site.xml -->
<property>
  <name>hadoop.security.credential.provider.path</name>
  <value></value>
  <!-- <value>localjceks://file/opt/mr3-run/key/hivemr3-ssl-certificate.jceks</value> -->
</property>

<!--
  Upon the deletion of DAGAppMasterPod, HiveServer2 tries to connect to DAGAppMaster Pod at least twice
  and up to three times (acknowledgeDagFinished(), getEstimateNumTasksOrNodes(), getApplicationReport().
  Each wait takes 20 seconds, so HiveServer2 may wait 3 * ipc.client.connect.max.retries.on.timeouts * 20 seconds
  before creating a new DAGAppMaster Pod.
 -->
<property>
  <name>ipc.client.connect.max.retries.on.timeouts</name>
  <value>3</value>
</property>

<!-- S3 -->

<!-- set when using S3 or on AWS EKS -->
<!-- options:
     com.amazonaws.auth.EnvironmentVariableCredentialsProvider
     com.amazonaws.auth.InstanceProfileCredentialsProvider
     com.amazonaws.auth.WebIdentityTokenCredentialsProvider -->
<property>
  <name>fs.s3a.aws.credentials.provider</name>
  <value>com.amazonaws.auth.WebIdentityTokenCredentialsProvider</value>
</property>

<property>
  <name>fs.s3a.connection.ssl.enabled</name>
  <value>false</value>
</property>

<!-- set when using S3 -->
<!-- do not set on AWS EKS -->
<property>
  <name>fs.s3a.endpoint</name>
  <value></value>
</property>

<!-- set to true when using path-style access to S3-compliant storage -->
<property>
  <name>fs.s3a.path.style.access</name>
  <value>true</value>
</property>

<property>
  <name>fs.s3a.impl</name>
  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
</property>
<property>
  <name>fs.s3.impl</name>
  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
</property>
<property>
  <name>fs.s3n.impl</name>
  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
</property>
<property>
  <name>fs.s3a.connection.maximum</name>
  <value>4000</value>
</property>

<property>
  <name>fs.s3.maxConnections</name>
  <value>4000</value>
</property>

<property>
  <name>fs.s3a.threads.max</name>
  <value>250</value>
</property>

<property>
  <name>fs.s3a.threads.core</name>
  <value>250</value>
</property>

<!-- S3 write performance -->

<property>
  <name>hive.mv.files.thread</name>
  <value>15</value>
</property>

<property>
  <name>fs.s3a.max.total.tasks</name>
  <value>5</value>
</property>

<property>
  <name>fs.s3a.blocking.executor.enabled</name>
  <value>false</value>
</property>

<!-- with HIVE-21390, the # of InputSplits is affected by hive.exec.orc.blob.storage.split.size
     when hive.exec.orc.split.strategy is set to BI -->
<property>
  <name>fs.s3a.block.size</name>
  <value>128M</value>
</property>

<!-- S3 input listing (Cf. hive.exec.input.listing.max.threads) -->
<property>
  <name>mapreduce.input.fileinputformat.list-status.num-threads</name>
  <value>50</value>
</property>

</configuration>
